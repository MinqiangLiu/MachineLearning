{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/2000], Loss = 0.168021\n",
      "Epoch [400/2000], Loss = 0.145609\n",
      "Epoch [600/2000], Loss = 0.130411\n",
      "Epoch [800/2000], Loss = 0.112941\n",
      "Epoch [1000/2000], Loss = 0.094166\n",
      "Epoch [1200/2000], Loss = 0.075462\n",
      "Epoch [1400/2000], Loss = 0.058259\n",
      "Epoch [1600/2000], Loss = 0.043693\n",
      "Epoch [1800/2000], Loss = 0.032418\n",
      "Epoch [2000/2000], Loss = 0.024340\n",
      "\n",
      "训练完成后最终Loss: 0.024340\n",
      "\n",
      "部分预测结果:\n",
      "X=[0.37454012 0.95071431], y_true=[1.25725195], y_pred=[1.23120023]\n",
      "X=[0.73199394 0.59865848], y_true=[1.3538778], y_pred=[1.27301047]\n",
      "X=[0.15601864 0.15599452], y_true=[0.34132041], y_pred=[0.50659318]\n",
      "X=[0.05808361 0.86617615], y_true=[0.85282462], y_pred=[0.91993813]\n",
      "X=[0.60111501 0.70807258], y_true=[1.49576504], y_pred=[1.24421276]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============ 1. 生成一些简单的假数据 ============\n",
    "# 这里用随机数来模拟输入数据 X 和标签 y\n",
    "# X 维度为 (N, 2)，其中 N 是样本数，每个样本有 2 个特征\n",
    "# y 维度为 (N, 1)，假设做一个简单的回归或二分类（0或1）\n",
    "np.random.seed(42)  # 固定随机种子，方便复现\n",
    "N = 100   # 样本数\n",
    "X = np.random.rand(N, 2)  # 生成随机输入\n",
    "# 生成一个简单的目标函数：y = x1 + x2 + 一点噪声\n",
    "y = X[:, [0]] + X[:, [1]] + 0.1 * np.random.randn(N, 1)\n",
    "\n",
    "# ============ 2. 初始化网络参数 ============\n",
    "# 假设单隐藏层含有 hidden_dim=3 个神经元\n",
    "input_dim = 2\n",
    "hidden_dim = 3\n",
    "output_dim = 1\n",
    "\n",
    "# 权重初始化可以用小随机数\n",
    "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)  # shape = [2, 3]\n",
    "b1 = np.zeros((1, hidden_dim))                     # shape = [1, 3]\n",
    "\n",
    "W2 = 0.1 * np.random.randn(hidden_dim, output_dim) # shape = [3, 1]\n",
    "b2 = np.zeros((1, output_dim))                     # shape = [1, 1]\n",
    "\n",
    "# ============ 3. 定义辅助函数（激活函数、损失函数等） ============\n",
    "\n",
    "# 1) 激活函数，这里使用 ReLU；也可以使用 sigmoid 或 tanh\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    # ReLU 的导数：x>0 时为1，x<=0 时为0\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# 2) 损失函数，这里以均方误差(MSE)为例\n",
    "def mean_squared_error(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "# ============ 4. 设置训练超参数 ============\n",
    "learning_rate = 0.01\n",
    "num_epochs = 2000  # 训练轮数\n",
    "\n",
    "# 为了观察训练过程中的损失走向，保存每一次迭代的loss\n",
    "loss_history = []\n",
    "\n",
    "# ============ 5. 训练循环 ============\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- (1) 前向传播 (Forward Pass) ----\n",
    "    # a1 = ReLU(X * W1 + b1)\n",
    "    z1 = np.dot(X, W1) + b1  # shape: (N, 3)\n",
    "    a1 = relu(z1)\n",
    "    \n",
    "    # y_pred = a1 * W2 + b2； 这里假设输出层不加激活函数(做回归)\n",
    "    z2 = np.dot(a1, W2) + b2  # shape: (N, 1)\n",
    "    y_pred = z2  # 对回归任务，最后一层不加激活\n",
    "\n",
    "    # ---- (2) 计算损失 ----\n",
    "    loss = mean_squared_error(y_pred, y)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # ---- (3) 反向传播 (Backpropagation) ----\n",
    "    # ① 计算输出层对 z2 的梯度 dL/dz2\n",
    "    # 对 MSE 来说，dL/dy_pred = (y_pred - y)，而 y_pred=z2，因此 dL/dz2 = 1/N * 2*(z2 - y) \n",
    "    # （不过常常省略常数因子 2 或者 1/N，和学习率一起调，不影响收敛）\n",
    "    dL_dz2 = (y_pred - y)  # shape: (N, 1)\n",
    "\n",
    "    # ② 计算对 W2 和 b2 的梯度\n",
    "    # dL/dW2 = a1^T * dL/dz2\n",
    "    dW2 = np.dot(a1.T, dL_dz2) / N  # 平均\n",
    "    db2 = np.sum(dL_dz2, axis=0, keepdims=True) / N\n",
    "\n",
    "    # ③ 传播到隐藏层：dL/da1 = dL/dz2 * dz2/da1\n",
    "    # 但 z2 = a1 * W2 + b2，只要按矩阵乘法反向即可\n",
    "    dL_da1 = np.dot(dL_dz2, W2.T)  # shape: (N, 3)\n",
    "\n",
    "    # ④ 计算对 z1 的梯度 dL/dz1 (考虑 ReLU 的导数)\n",
    "    dL_dz1 = dL_da1 * relu_deriv(z1)  # shape: (N, 3)\n",
    "\n",
    "    # ⑤ 计算对 W1 和 b1 的梯度\n",
    "    dW1 = np.dot(X.T, dL_dz1) / N  # 平均\n",
    "    db1 = np.sum(dL_dz1, axis=0, keepdims=True) / N\n",
    "\n",
    "    # ---- (4) 参数更新 (Gradient Descent) ----\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    # ---- (5) 打印训练过程 ----\n",
    "    if (epoch+1) % 200 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss = {loss:.6f}\")\n",
    "\n",
    "# ============ 6. 训练完成后的结果 ============\n",
    "print(\"\\n训练完成后最终Loss: {:.6f}\".format(loss_history[-1]))\n",
    "\n",
    "# 这里我们可以查看任意几条样本的预测输出与真实值对比\n",
    "test_indices = [0, 1, 2, 3, 4]\n",
    "print(\"\\n部分预测结果:\")\n",
    "for i in test_indices:\n",
    "    print(f\"X={X[i]}, y_true={y[i]}, y_pred={y_pred[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

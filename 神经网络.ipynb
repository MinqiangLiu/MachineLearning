{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200, Loss: 0.246401\n",
      "Epoch: 400, Loss: 0.246400\n",
      "Epoch: 600, Loss: 0.246400\n",
      "Epoch: 800, Loss: 0.246400\n",
      "Epoch: 1000, Loss: 0.246400\n",
      "Epoch: 1200, Loss: 0.246400\n",
      "Epoch: 1400, Loss: 0.246400\n",
      "Epoch: 1600, Loss: 0.246400\n",
      "Epoch: 1800, Loss: 0.246400\n",
      "Epoch: 2000, Loss: 0.246400\n",
      "\n",
      "Final training loss: 0.24640000000361428\n",
      "\n",
      "Sample predictions:\n",
      "X[0]: Label=1, Pred=0.4400\n",
      "X[1]: Label=1, Pred=0.4400\n",
      "X[2]: Label=1, Pred=0.4400\n",
      "X[3]: Label=1, Pred=0.4400\n",
      "X[4]: Label=0, Pred=0.4400\n",
      "X[5]: Label=1, Pred=0.4400\n",
      "X[6]: Label=0, Pred=0.4400\n",
      "X[7]: Label=1, Pred=0.4400\n",
      "X[8]: Label=0, Pred=0.4400\n",
      "X[9]: Label=1, Pred=0.4400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============= 1. 超参数设定 =============\n",
    "N = 200           # 训练样本数，可以根据需要调整\n",
    "input_dim = 128   # 输入层神经元数（特征数）\n",
    "hidden_size = 64  # 每个隐藏层神经元数，可自由修改\n",
    "num_hidden_layers = 5  # 隐藏层层数\n",
    "output_dim = 1    # 输出层神经元数（单神经元，Sigmoid，用于二分类）\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 2000\n",
    "\n",
    "# ============= 2. 生成随机数据集 =============\n",
    "# X: (N, 128)\n",
    "X = np.random.randn(N, input_dim)\n",
    "\n",
    "# y: (N, 1)  二分类标签(0或1)，这里用随机生成的示例数据\n",
    "y = np.random.randint(0, 2, size=(N, 1))  # 随机标签 0/1\n",
    "\n",
    "# ============= 3. 初始化权重和偏置 =============\n",
    "# 为简化存储，将所有层的权重、偏置放进列表中。\n",
    "# layers: [input_dim, hidden_size, hidden_size, ..., output_dim]\n",
    "layer_dims = [input_dim] + [hidden_size]*num_hidden_layers + [output_dim]\n",
    "\n",
    "# 定义一个函数来随机初始化每层的 W、b\n",
    "def init_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    根据每一层的输入输出维度初始化网络参数\n",
    "    layer_dims: [l0, l1, l2, ..., l_{L}]\n",
    "    返回:\n",
    "      W_list: [W1, W2, ...]\n",
    "      b_list: [b1, b2, ...]\n",
    "    其中:\n",
    "      Wk 形状: (layer_dims[k], layer_dims[k+1])\n",
    "      bk 形状: (layer_dims[k+1], )\n",
    "    \"\"\"\n",
    "    W_list = []\n",
    "    b_list = []\n",
    "    for i in range(len(layer_dims) - 1):\n",
    "        W = np.random.randn(layer_dims[i], layer_dims[i+1]) * 0.01\n",
    "        b = np.zeros((layer_dims[i+1],))\n",
    "        W_list.append(W)\n",
    "        b_list.append(b)\n",
    "    return W_list, b_list\n",
    "\n",
    "W_list, b_list = init_parameters(layer_dims)\n",
    "\n",
    "# ============= 4. 定义激活函数及其导数 =============\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    # a = sigmoid(z)，则 a' = a(1-a)\n",
    "    return a * (1 - a)\n",
    "\n",
    "# ============= 5. 开始训练 =============\n",
    "for epoch in range(epochs):\n",
    "    # ------- 前向传播 --------\n",
    "    # 缓存每层输出(激活值)和 z 值，方便反向传播\n",
    "    a_cache = [X]  # a_cache[0] = X\n",
    "    z_cache = []\n",
    "    \n",
    "    # forward pass\n",
    "    current_a = X  # 当前层的输出（从输入层开始）\n",
    "    for l in range(len(W_list)):\n",
    "        z = np.dot(current_a, W_list[l]) + b_list[l]  # (N, layer_dims[l+1])\n",
    "        a = sigmoid(z)\n",
    "        \n",
    "        z_cache.append(z)\n",
    "        a_cache.append(a)\n",
    "        \n",
    "        current_a = a  # 传给下一层输入\n",
    "    \n",
    "    # 当前层输出即网络最终输出 y_pred\n",
    "    y_pred = current_a  # shape: (N, 1)\n",
    "    \n",
    "    # ------- 计算损失 (MSE) --------\n",
    "    loss = np.mean((y_pred - y)**2)\n",
    "    \n",
    "    # ------- 反向传播 --------\n",
    "    # dLoss/dy_pred, 对 MSE 来说 = 2*(y_pred - y)/N\n",
    "    dLoss_dy = 2.0 * (y_pred - y) / N\n",
    "    \n",
    "    # 我们需要逐层计算 dLoss/dWk、dLoss/dbk\n",
    "    # L 层网络，对应 len(W_list) = L\n",
    "    # 逆序遍历每一层\n",
    "    dA = dLoss_dy  # 先对最后一层的输出进行求导\n",
    "    for l in reversed(range(len(W_list))):\n",
    "        # a_l   = a_cache[l]\n",
    "        # z_l   = z_cache[l]\n",
    "        # a_{l+1} = a_cache[l+1]  => y_pred\n",
    "        # W_l   = W_list[l]\n",
    "        \n",
    "        # 先计算 dZ = dA * sigmoid'(Z)\n",
    "        dZ = dA * sigmoid_derivative(a_cache[l+1])  # shape: (N, layer_dims[l+1])\n",
    "        \n",
    "        # dW = a_l^T dot dZ\n",
    "        dW = np.dot(a_cache[l].T, dZ)  # shape: (layer_dims[l], layer_dims[l+1])\n",
    "        \n",
    "        # db = 对 dZ 在样本维度求和\n",
    "        db = np.sum(dZ, axis=0)  # shape: (layer_dims[l+1], )\n",
    "        \n",
    "        # 更新权重\n",
    "        W_list[l] -= learning_rate * dW\n",
    "        b_list[l] -= learning_rate * db\n",
    "        \n",
    "        # 计算传回到前一层的 dA (如果还有前一层的话)\n",
    "        # dA_{l-1} = dZ dot W_list[l]^T\n",
    "        dA = np.dot(dZ, W_list[l].T)\n",
    "    \n",
    "    # 可选: 每隔一段打印一下损失，观察收敛情况\n",
    "    if (epoch+1) % 200 == 0:\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {loss:.6f}\")\n",
    "\n",
    "# ============= 6. 测试: 看最终输出分布 =============\n",
    "print(\"\\nFinal training loss:\", loss)\n",
    "# 这里简单打印前 10 个预测结果对比\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(10):\n",
    "    print(f\"X[{i}]: Label={y[i,0]}, Pred={y_pred[i,0]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

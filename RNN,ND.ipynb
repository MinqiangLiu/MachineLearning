{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss = 2.276755\n",
      "Epoch 20/100, Loss = 0.747057\n",
      "Epoch 30/100, Loss = 0.065337\n",
      "Epoch 40/100, Loss = 0.021704\n",
      "Epoch 50/100, Loss = 0.009755\n",
      "Epoch 60/100, Loss = 0.004976\n",
      "Epoch 70/100, Loss = 0.002936\n",
      "Epoch 80/100, Loss = 0.002053\n",
      "Epoch 90/100, Loss = 0.001667\n",
      "Epoch 100/100, Loss = 0.001493\n",
      "\n",
      "实际输出 y_data[:10]:\n",
      "[ 0.44806111 -0.79065823 -0.22184356 -0.31010667 -0.25282217  0.19944143\n",
      " -0.01505699 -0.04773612  0.10144743 -0.14919139]\n",
      "预测输出 y_pred[:10]:\n",
      "[ 0.45157019 -0.75982746 -0.22614844 -0.32124421 -0.25618755  0.2076185\n",
      " -0.0197995  -0.04907385  0.10088893 -0.15778372]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    这是一个最简循环神经网络 (RNN) 的实现示例，用于演示基本的前向传播和反向传播流程。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):\n",
    "        \"\"\"\n",
    "        RNN 参数初始化\n",
    "\n",
    "        参数：\n",
    "        - input_dim:  输入数据的维度\n",
    "        - hidden_dim: 隐藏层维度\n",
    "        - output_dim: 输出层维度\n",
    "        - lr:         学习率（learning rate）\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "\n",
    "        # 初始化权重：输入 -> 隐藏\n",
    "        # shape = (input_dim, hidden_dim)\n",
    "        self.W_xh = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "\n",
    "        # 初始化权重：隐藏 -> 隐藏（即循环部分）\n",
    "        # shape = (hidden_dim, hidden_dim)\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "\n",
    "        # 初始化权重：隐藏 -> 输出\n",
    "        # shape = (hidden_dim, output_dim)\n",
    "        self.W_hy = np.random.randn(hidden_dim, output_dim) * 0.01\n",
    "\n",
    "        # 初始化偏置：隐藏层和输出层\n",
    "        # shape = (hidden_dim,)\n",
    "        self.b_h = np.zeros((hidden_dim, ))\n",
    "        # shape = (output_dim,)\n",
    "        self.b_y = np.zeros((output_dim, ))\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        \"\"\"\n",
    "        RNN 的前向传播。\n",
    "        给定序列 x_seq（形状 (T, input_dim)），\n",
    "        依次计算每个时间步 t 的隐藏状态 h(t) 和输出 y(t)。\n",
    "\n",
    "        返回:\n",
    "        - h_seq: 每个时间步的隐藏状态数组 (T, hidden_dim)\n",
    "        - y_seq: 每个时间步的输出数组 (T, output_dim)\n",
    "        \"\"\"\n",
    "        T = x_seq.shape[0]  # 序列长度\n",
    "        # 初始化隐藏状态 h(0)，设为 0 向量\n",
    "        h_t = np.zeros((self.hidden_dim, ))\n",
    "\n",
    "        # 用来收集每个时间步的隐藏状态和输出\n",
    "        h_seq = []\n",
    "        y_seq = []\n",
    "\n",
    "        for t in range(T):\n",
    "            # 取出第 t 个时间步的输入 x(t)，形状 (input_dim, )\n",
    "            x_t = x_seq[t]  # shape = (input_dim, )\n",
    "\n",
    "            # 1) 计算隐藏状态 h(t) = tanh(x(t) * W_xh + h(t-1) * W_hh + b_h)\n",
    "            h_t = np.tanh(\n",
    "                x_t @ self.W_xh +\n",
    "                h_t @ self.W_hh +\n",
    "                self.b_h\n",
    "            )\n",
    "            h_seq.append(h_t)\n",
    "\n",
    "            # 2) 计算输出 y(t) = h(t) * W_hy + b_y\n",
    "            y_t = h_t @ self.W_hy + self.b_y\n",
    "            y_seq.append(y_t)\n",
    "\n",
    "        # 转成 numpy 数组\n",
    "        h_seq = np.stack(h_seq, axis=0)  # (T, hidden_dim)\n",
    "        y_seq = np.stack(y_seq, axis=0)  # (T, output_dim)\n",
    "\n",
    "        return h_seq, y_seq\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        使用均方误差 (MSE) 计算损失。\n",
    "\n",
    "        参数：\n",
    "        - y_pred: 模型预测输出，形状 (T, output_dim)\n",
    "        - y_true: 真实目标输出，形状 (T, output_dim)\n",
    "\n",
    "        返回:\n",
    "        - MSE 损失值 (float)\n",
    "        \"\"\"\n",
    "        return 0.5 * np.sum((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self, x_seq, h_seq, y_seq, y_true):\n",
    "        \"\"\"\n",
    "        通过时间的反向传播 (BPTT)，计算并更新各参数梯度。\n",
    "\n",
    "        参数：\n",
    "        - x_seq:  输入序列 (T, input_dim)\n",
    "        - h_seq:  每个时间步的隐藏状态 (T, hidden_dim)\n",
    "        - y_seq:  每个时间步的预测输出 (T, output_dim)\n",
    "        - y_true: 每个时间步的真实输出 (T, output_dim)\n",
    "        \"\"\"\n",
    "        T = x_seq.shape[0]\n",
    "\n",
    "        # 初始化各参数的梯度\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h  = np.zeros_like(self.b_h)\n",
    "        db_y  = np.zeros_like(self.b_y)\n",
    "\n",
    "        # dh_next 用于存放来自后面时间步的隐藏状态梯度\n",
    "        dh_next = np.zeros((self.hidden_dim, ))\n",
    "\n",
    "        # 从最后一个时间步往前进行 BPTT\n",
    "        for t in reversed(range(T)):\n",
    "            # ============== 第 1 步：计算输出层梯度 ==============\n",
    "            dy = (y_seq[t] - y_true[t])  # shape = (output_dim, )\n",
    "            # 对 W_hy 和 b_y 的梯度\n",
    "            dW_hy += np.outer(h_seq[t], dy)  # (hidden_dim, ) -> (hidden_dim, output_dim)\n",
    "            db_y  += dy                      \n",
    "\n",
    "            # ============== 第 2 步：累加对隐藏状态的梯度 ==============\n",
    "            dh = dy @ self.W_hy.T + dh_next  # (hidden_dim, )\n",
    "            \n",
    "            # 由于 h(t) = tanh(z)，z = x(t)*W_xh + h(t-1)*W_hh + b_h\n",
    "            # tanh'(z) = 1 - tanh^2(z)\n",
    "            # 因此对 z 的梯度为 dh * (1 - h(t)^2)\n",
    "            dtanh = (1 - h_seq[t]**2) * dh\n",
    "\n",
    "            # ============== 第 3 步：计算对参数的梯度 ==============\n",
    "            # (1) 对 W_xh 的梯度\n",
    "            x_t = x_seq[t]  # shape = (input_dim, )\n",
    "            dW_xh += np.outer(x_t, dtanh)  # (input_dim, hidden_dim)\n",
    "\n",
    "            # (2) 对 W_hh 的梯度\n",
    "            h_prev = np.zeros_like(h_seq[t]) if t == 0 else h_seq[t-1]\n",
    "            dW_hh += np.outer(h_prev, dtanh)  # (hidden_dim, hidden_dim)\n",
    "\n",
    "            # (3) 对偏置 b_h 的梯度\n",
    "            db_h  += dtanh  # shape = (hidden_dim, )\n",
    "\n",
    "            # ============== 第 4 步：计算对前一个隐藏状态的梯度 ==============\n",
    "            dh_next = dtanh @ self.W_hh.T  # shape = (hidden_dim, )\n",
    "\n",
    "        # ============== 第 5 步：参数更新 (梯度下降) ==============\n",
    "        self.W_xh -= self.lr * dW_xh\n",
    "        self.W_hh -= self.lr * dW_hh\n",
    "        self.W_hy -= self.lr * dW_hy\n",
    "        self.b_h  -= self.lr * db_h\n",
    "        self.b_y  -= self.lr * db_y\n",
    "\n",
    "    def train_step(self, x_seq, y_true):\n",
    "        \"\"\"\n",
    "        进行一次完整的前向传播、损失计算和反向传播，返回当前损失值。\n",
    "\n",
    "        参数：\n",
    "        - x_seq:  输入序列 (T, input_dim)\n",
    "        - y_true: 目标序列 (T, output_dim)\n",
    "\n",
    "        返回:\n",
    "        - 当前的损失值 (float)\n",
    "        \"\"\"\n",
    "        # 1) 前向传播\n",
    "        h_seq, y_seq = self.forward(x_seq)\n",
    "\n",
    "        # 2) 计算损失\n",
    "        loss = self.compute_loss(y_seq, y_true)\n",
    "\n",
    "        # 3) 反向传播\n",
    "        self.backward(x_seq, h_seq, y_seq, y_true)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# =========== 使用示例 ===========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置随机种子，便于复现实验结果\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # 设定超参数\n",
    "    T = 50           # 序列长度\n",
    "    input_dim = 10   # 输入维度\n",
    "    hidden_dim = 16  # 隐藏层维度（可自行调整）\n",
    "    output_dim = 1   # 输出层维度（此处假设做回归或一个数值的预测）\n",
    "    lr = 0.01        # 学习率\n",
    "    num_epochs = 100 # 训练轮数\n",
    "\n",
    "    # 构造随机输入和目标\n",
    "    # x_data: (T, input_dim)\n",
    "    x_data = np.random.randn(T, input_dim)\n",
    "    # 假设目标 y_data = x_data 在第一个维度上的平均值（或其他简单映射），仅作示例\n",
    "    y_data = np.mean(x_data, axis=1, keepdims=True)  # shape = (T, 1)\n",
    "\n",
    "    # 初始化 RNN\n",
    "    rnn = SimpleRNN(input_dim, hidden_dim, output_dim, lr=lr)\n",
    "\n",
    "    # 训练\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_val = rnn.train_step(x_data, y_data)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss = {loss_val:.6f}\")\n",
    "\n",
    "    # 最终预测\n",
    "    _, y_pred = rnn.forward(x_data)\n",
    "    print(\"\\n实际输出 y_data[:10]:\")\n",
    "    print(y_data[:10].flatten())\n",
    "    print(\"预测输出 y_pred[:10]:\")\n",
    "    print(y_pred[:10].flatten())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
